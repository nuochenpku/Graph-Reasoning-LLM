{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/cpfs/user/chennuo/CN/NLGraph/NLGraph/cycle/main.json') as f:\n",
    "    datas = json.load(f)\n",
    "print(len(datas))\n",
    "datas['120']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import time\n",
    "# openai.api_type = \"azure\"\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://new-llm.openai.azure.com/\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = \"082b19bc29364b1bb39d2d9fb9b757d4\"\n",
    "def chatGPT_test(text, content=None):\n",
    "\n",
    "    prompt = [{\"role\": \"system\", \"content\": 'You are a helpful assistant that are good at providing detailed explanations to solve graph reasoning tasks.'},\n",
    "        {\"role\": \"user\", \"content\": \"{}\".format(text)}]\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            if i>=3:\n",
    "                break\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                engine=\"gpt-35-turbo\",\n",
    "                messages = prompt,\n",
    "                temperature=0.0,\n",
    "                max_tokens=300,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stop='\\n')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            i+=1\n",
    "            tqdm.write(str(e))\n",
    "            tqdm.write(\"Retrying...\")\n",
    "            # import time\n",
    "            time.sleep(30)\n",
    "    time.sleep(10.0)\n",
    "    return completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "with open(\"/cpfs/user/chennuo/CN/NLGraph/NLGraph/connectivity/prompt/\" + \"CoT-prompt.txt\", \"r\") as f:\n",
    "    exemplar = f.read()\n",
    "\n",
    "with open('/cpfs/user/chennuo/CN/NLGraph/NLGraph/connectivity/train.json') as f:\n",
    "    datas = json.load(f)\n",
    "\n",
    "gen_datas_jsonl = '/cpfs/user/chennuo/CN/NLGraph/NLGraph/connectivity/chatgpt/_generate_train.json'\n",
    "   \n",
    "start_index = (\n",
    "        len(open(gen_datas_jsonl).readlines()) if gen_datas_jsonl.exists() else 0\n",
    "    )\n",
    "print(f\"start_index: {start_index}\")\n",
    "    \n",
    "for key, value in tqdm(datas.items()):\n",
    "    \n",
    "    if int(key) <= start_index:\n",
    "        continue\n",
    "    \n",
    "    query = value['question']\n",
    "    instruct = query.split('\\n')[0] + \" Begin with '###' to give your final conclusion.\"\n",
    "    query = query.replace(query.split('\\n')[0], instruct)\n",
    "    # query += \"\\nLet's think step by step:\"\n",
    "    inputs = exemplar + '\\n\\n\\n' + query\n",
    "    try:\n",
    "        response = chatGPT_test(inputs)\n",
    "        new_data = dict()\n",
    "        value['chatgpt_response'] = response\n",
    "        new_data[key] = value\n",
    "        with open('/cpfs/user/chennuo/CN/NLGraph/NLGraph/connectivity/chatgpt/_generate_train.json', \"a\") as w:\n",
    "            w.write(json.dumps(new_data, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "        if 'yes' in value['answer'] and 'yes' in response.split('###')[-1].lower():\n",
    "            with open('/cpfs/user/chennuo/CN/NLGraph/NLGraph/connectivity/chatgpt/_generate_train_correct.json', \"a\") as w:\n",
    "                w.write(json.dumps(new_data, ensure_ascii=False) + '\\n')\n",
    "                \n",
    "        elif 'no' in value['answer'] and 'no' in response.split('###')[-1].lower():\n",
    "            with open('/cpfs/user/chennuo/CN/NLGraph/NLGraph/connectivity/chatgpt/_generate_train_correct.json', \"a\") as w:\n",
    "                w.write(json.dumps(new_data, ensure_ascii=False) + '\\n')\n",
    "        else:\n",
    "            with open('/cpfs/user/chennuo/CN/NLGraph/NLGraph/connectivity/chatgpt/_generate_train_wrong.json', \"a\") as w:\n",
    "                w.write(json.dumps(new_data, ensure_ascii=False) + '\\n')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18064"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/cpfs/user/chennuo/CN/NLGraph/NLGraph/GPT4_Gen/gpt4_nlg_sample3_res.json') as f:\n",
    "    datas = f.readlines()\n",
    "import json\n",
    "json.loads(datas[0])\n",
    "len(datas)\n",
    "# merge_ = {}\n",
    "# for data in datas:\n",
    "#     data = json.loads(data)\n",
    "#     if data['query'] not in merge_:\n",
    "#         merge_[data['query']].append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1861/1861 [09:41<00:00,  3.20it/s]\n",
      "100%|██████████| 371/371 [01:59<00:00,  3.10it/s]\n",
      "100%|██████████| 959/959 [05:06<00:00,  3.12it/s]\n",
      "100%|██████████| 191/191 [00:59<00:00,  3.21it/s]\n",
      "100%|██████████| 292/292 [01:32<00:00,  3.17it/s]\n",
      "100%|██████████| 58/58 [00:18<00:00,  3.20it/s]\n",
      "100%|██████████| 201/201 [01:02<00:00,  3.20it/s]\n",
      "100%|██████████| 39/39 [00:12<00:00,  3.19it/s]\n",
      "100%|██████████| 292/292 [01:31<00:00,  3.19it/s]\n",
      "100%|██████████| 58/58 [00:18<00:00,  3.21it/s]\n",
      "100%|██████████| 426/426 [02:12<00:00,  3.21it/s]\n",
      "100%|██████████| 84/84 [00:25<00:00,  3.26it/s]\n",
      "100%|██████████| 316/316 [01:37<00:00,  3.25it/s]\n",
      "100%|██████████| 64/64 [00:19<00:00,  3.22it/s]\n",
      "100%|██████████| 675/675 [03:30<00:00,  3.21it/s]\n",
      "100%|██████████| 135/135 [00:41<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "tasks = ['connectivity', 'cycle','flow', 'GNN', 'hamilton', 'matching','shortest_path', 'topology']\n",
    "tasks_train_samples = {task:[] for task in tasks}\n",
    "tasks_test_samples = {task:[] for task in tasks}\n",
    "import json\n",
    "with open('/cpfs/user/chennuo/CN/NLGraph/NLGraph/GPT4_Gen/gpt4_nlg_sample3_res.json') as f:\n",
    "    datas = f.readlines()\n",
    "from tqdm import tqdm\n",
    "for task in tasks:\n",
    "    with open(f'/cpfs/user/chennuo/CN/NLGraph/NLGraph/{task}/train.json') as f:\n",
    "        t_datas = json.load(f)\n",
    "    t_len = len(t_datas)\n",
    "    for key, data in tqdm(t_datas.items()):\n",
    "        response = data['answer']    \n",
    "        query = data['question']\n",
    "        for data in datas:\n",
    "            data = json.loads(data)\n",
    "            if data['query'] == query:\n",
    "                tasks_train_samples[task].append(data)\n",
    "                # break\n",
    "            \n",
    "    with open(f'/cpfs/user/chennuo/CN/NLGraph/NLGraph/{task}/test.json') as f:\n",
    "        t_datas = json.load(f)\n",
    "    t_len = len(t_datas)\n",
    "    for key, data in tqdm(t_datas.items()):\n",
    "        response = data['answer']    \n",
    "        query = data['question']\n",
    "        for data in datas:\n",
    "            data = json.loads(data)\n",
    "            if data['query'] == query:\n",
    "                tasks_test_samples[task].append(data)\n",
    "                # break\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5588"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tasks_train_samples['connectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_samples  = {task:[] for task in tasks}\n",
    "import re\n",
    "def extract_last_num(text: str) -> float:\n",
    "    text = re.sub(r\"(\\d),(\\d)\", \"\\g<1>\\g<2>\", text)  # 处理形如 123,456\n",
    "    res = re.findall(r\"(\\d+(\\.\\d+)?)\", text)  # 匹配 123456.789\n",
    "    if len(res) > 0:\n",
    "        num_str = res[-1][0]\n",
    "        return float(num_str)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "for key, value in tasks_train_samples.items():\n",
    "    \n",
    "    if key in ['cycle', 'connectivity']:\n",
    "        querys = []\n",
    "        for i, v in enumerate(value):\n",
    "            truth = v['response']\n",
    "            v['id'] = i\n",
    "            if 'yes' in truth.lower() and 'yes' in v['dv3_response'].split('###')[-1].lower():\n",
    "                if v['query'] not in querys:\n",
    "                    correct_samples[key].append(v)\n",
    "                    querys.append(v['query'])\n",
    "                # else:\n",
    "                    \n",
    "            elif 'no' in truth.lower() and 'no' in v['dv3_response'].split('###')[-1].lower():\n",
    "                if v['query'] not in querys:\n",
    "                    correct_samples[key].append(v)\n",
    "                    querys.append(v['query'])\n",
    "                \n",
    "    elif key == 'flow':\n",
    "        querys = []\n",
    "        for i, v in enumerate(value):\n",
    "            truth = v['response']\n",
    "            v['id'] = i\n",
    "            t_num = extract_last_num(truth)\n",
    "            p_num = extract_last_num(v['dv3_response'].split('###')[-1])\n",
    "            if abs(t_num - p_num) < 1e-2:\n",
    "                if v['query'] not in querys:\n",
    "                    correct_samples[key].append(v)\n",
    "                    querys.append(v['query'])\n",
    "                # correct_samples[key].append(v)\n",
    "                \n",
    "    elif key == 'hamilton':\n",
    "        querys = []\n",
    "        for i, v in enumerate(value):\n",
    "            truth = v['response'].split('be: ')[-1].strip('.')\n",
    "            v['id'] = i\n",
    "            if truth in v['dv3_response']:\n",
    "                if v['query'] not in querys:\n",
    "                    correct_samples[key].append(v)\n",
    "                    querys.append(v['query'])\n",
    "                # correct_samples[key].append(v)\n",
    "                \n",
    "    elif key == 'matching':\n",
    "        querys = []\n",
    "        for i, v in enumerate(value):\n",
    "            truth = v['response'].split('\\n')[-1]\n",
    "            v['id'] = i\n",
    "            t_num = extract_last_num(truth)\n",
    "            p_num = extract_last_num(v['dv3_response'].split('###')[-1])\n",
    "            if abs(t_num - p_num) < 1e-2:\n",
    "                if v['query'] not in querys:\n",
    "                    correct_samples[key].append(v)\n",
    "                    querys.append(v['query'])\n",
    "                \n",
    "    elif key == 'shortest_path':\n",
    "        querys = []\n",
    "        for i, v in enumerate(value):\n",
    "            truth = v['response'].split('is')[-1].split('with')[0].strip(' ')\n",
    "            v['id'] = i\n",
    "            # t_num = extract_last_num(truth)\n",
    "            if truth in v['dv3_response']:\n",
    "                if v['query'] not in querys:\n",
    "                    correct_samples[key].append(v)\n",
    "                    querys.append(v['query'])\n",
    "                \n",
    "    elif key == 'topology':\n",
    "        querys = []\n",
    "        for i, v in enumerate(value):\n",
    "            truth = v['response'].split('is: ')[-1].strip(' ').strip('.')\n",
    "            v['id'] = i\n",
    "            # t_num = extract_last_num(truth)\n",
    "            if truth in v['dv3_response']:\n",
    "                if v['query'] not in querys:\n",
    "                    correct_samples[key].append(v)\n",
    "                    querys.append(v['query'])\n",
    "            # if i <=5:\n",
    "            #     print(truth)\n",
    "            #     print(v['dv3_response'])\n",
    "            #     print('&&&&&&&&&')\n",
    "                \n",
    "    elif key == 'GNN':\n",
    "        querys = []\n",
    "        for i, v in enumerate(value):\n",
    "            truths = v['response'].split('\\n')[1:-1]\n",
    "            v['id'] = i\n",
    "            # t_num = extract_last_num(truth)\n",
    "            flag = True\n",
    "            for truth in truths:\n",
    "                if truth not in v['dv3_response'].split('###')[-1]:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                if v['query'] not in querys:\n",
    "                    correct_samples[key].append(v)\n",
    "                    querys.append(v['query'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connectivity\n",
      "1813\n",
      "cycle\n",
      "780\n",
      "flow\n",
      "83\n",
      "GNN\n",
      "70\n",
      "hamilton\n",
      "14\n",
      "matching\n",
      "416\n",
      "shortest_path\n",
      "271\n",
      "topology\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "for key, value in correct_samples.items():\n",
    "    print(key)\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connectivity\n",
      "1697\n",
      "cycle\n",
      "592\n",
      "flow\n",
      "45\n",
      "GNN\n",
      "63\n",
      "hamilton\n",
      "6\n",
      "matching\n",
      "393\n",
      "shortest_path\n",
      "200\n",
      "topology\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "for key, value in correct_samples.items():\n",
    "    print(key)\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in  tasks_train_samples.items():\n",
    "    print(key)\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "with open('/cpfs/user/chennuo/CN/XMATH-LLM/data/NLGGraph/train_v2.json', 'w')  as writer, open('/cpfs/user/chennuo/CN/XMATH-LLM/data/NLGGraph/test_v2.json', 'w')  as writer1:\n",
    "   for key, value in correct_samples.items():\n",
    "        # print(key)\n",
    "        # print(len(value)) \n",
    "        for i, v in enumerate(value):\n",
    "            new_data = dict()\n",
    "            query = v['query'].split('\\nA:')[0].replace('\\nQ:','\\n###Question:')\n",
    "            new_data['prompt'] = PROMPT_DICT[\"prompt_no_input\"].format(instruction=query)\n",
    "            new_data['chosen'] = v['dv3_response'].split('Possible answer:\\n\\n')[-1]\n",
    "            new_data['reject'] = \"I DO NOT KNOW.\"\n",
    "            if i <=1:\n",
    "                writer1.write(json.dumps(new_data, ensure_ascii=False) + '\\n')\n",
    "            else:\n",
    "                writer.write(json.dumps(new_data, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ans = np.load('/cpfs/user/chennuo/CN/NLGraph/NLGraph/hamilton/graph/hard/log/20230501---17-44-CoT/answer.npy')\n",
    "res = np.load('/cpfs/user/chennuo/CN/NLGraph/NLGraph/hamilton/graph/hard/log/20230501---17-44-CoT/res.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "stores = np.load('/cpfs/user/chennuo/dsChatLLama/test_data/results/retrieval/sentence_embeddings.npy', allow_pickle=True)\n",
    "# stores['vtD61B1EE5AB3D4E54A764481B2D05B8FC-ac49A5EB7BA51D42479C149082AF731293']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5084, -0.2955, -0.8260,  ...,  0.4785,  0.1249, -0.2732],\n",
       "        [ 1.1506,  0.4685, -0.8244,  ...,  0.3458,  0.3433, -0.3221],\n",
       "        [ 0.1429, -0.5485, -0.3346,  ...,  0.7254, -1.0859, -0.4258],\n",
       "        ...,\n",
       "        [ 0.9380, -0.0750, -1.1993,  ...,  0.9142, -0.4972, -0.6364],\n",
       "        [-0.4368,  0.5446, -0.8948,  ...,  0.2306,  0.6288,  0.3295],\n",
       "        [ 1.0552,  0.3792, -1.2823,  ...,  1.0564,  0.5365, -0.2187]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores.item()['vtD61B1EE5AB3D4E54A764481B2D05B8FC-ac49A5EB7BA51D42479C149082AF731293']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\\nThe nodes are numbered from 0 to 8, and the edges are: (0,4) (0,2) (0,6) (0,7) (0,1) (1,5) (2,3) (2,6) (2,5) (3,4) (3,7) (4,7) (4,6) (5,6) (5,7) (6,8) (7,8)\\nQ: Is there a path in this graph that visits every node exactly once? If yes, give the path. Note that in a path, adjacent nodes must be connected with edges.\\nA:',\n",
       " 'answer': 'Yes. The path can be: 0,1,5,7,8,6,4,3,2',\n",
       " 'difficulty': 'easy'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('/cpfs/user/chennuo/CN/NLGraph/NLGraph/hamilton/test.json') as f:\n",
    "    datas = json.load(f)\n",
    "datas['0']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
